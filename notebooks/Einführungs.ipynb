{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìå Was ist eine Loss Function / Verlustfunktion?",
   "id": "cfc4e97f8721b324"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Die **Loss Function (Verlustfunktion)** ist das Herzst√ºck des Lernprozesses im Deep Learning.\n",
    "\n",
    "Die Loss Function misst, wie falsch das Modell aktuell liegt.\n",
    "**Das Ziel des Trainings ist es, den Loss zu minimieren,** also:\n",
    "\n",
    "\"Die Vorhersagen des Modells sollen **immer n√§her an die wahren Labels kommen.\"**\n",
    "### Binary Entropy Loss ? (BCELoss)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "loss = criterion(y_pred, y_true)\n",
    "\n",
    "Pytorch modeul , Standard bei bin√§rer Klassifikation (0 vs. 1)\n",
    "\n",
    "Verwendete Formel der Binary Cross Entropy:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Mit Mask : 1\n",
    "\n",
    "Ohne Mask : 0 \n",
    "\n",
    "$$\n",
    "L(\\hat{o}, (i, o)) \\in [0, \\infty)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Loss} = - \\left[ y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) \\right]\n",
    "$$\n",
    "\n",
    "Dabei ist:\n",
    "- \\( y \\in \\{0, 1\\} \\) das echte Label  \n",
    "- \\( p \\in [0,1] \\) die vom Modell vorhergesagte Wahrscheinlichkeit\n",
    "\n",
    "\n",
    "# üß† Entscheidungsfunktion: Theorie & Definition\n",
    "\n",
    "Die **Entscheidungsfunktion** sagt aus, welche Ausgabe das Modell f√ºr eine bestimmte Eingabe produziert.\n",
    "Formal gesagt:\n",
    "\n",
    "Eine Funktion \n",
    "ùõø\n",
    "Œ¥, die jedem Eingabevektor \n",
    "ùëñ\n",
    "i aus dem Merkmalsraum \n",
    "ùëÜ\n",
    "S eine Entscheidung \n",
    "ùëú\n",
    "o aus dem Ausgaberaum \n",
    "ùëÇ\n",
    "O zuordnet.\n",
    "\n",
    "$$\n",
    "\\delta : \\mathcal{S} \\rightarrow O\n",
    "$$\n",
    "\n",
    "- \\(S ‚äÜ{R}^n\\) ist der Merkmalsraum (Input)\n",
    "- \\(O ‚äÜ{R}\\) ist der Ausgaberaum (z.‚ÄØB. {0, 1})\n",
    "\n",
    "Im Kontext der Maskenerkennung:\n",
    "\n",
    "- Eingabe \\(i\\): Bilddaten eines Gesichts\n",
    "- Ausgabe \\(Œ¥(i)\\): Entscheidung = ‚ÄûMaske‚Äú (1) oder ‚Äûkeine Maske‚Äú (0)\n",
    "\n",
    "‚ö†Ô∏è Die Ausgabe \\(Œ¥(i)\\) kann sich von der wahren Ausgabe \\(o\\) unterscheiden.\n",
    "\n",
    "\n",
    "**Das bedeutet Zwei Klassen und eine Entscheidung**\n",
    "\n",
    "\n",
    "**Entscheidungsfunktion (Œ¥)  ‚Üí  Verlustfunktion (L)  ‚Üí  BCE-Formel  ‚Üí  Loss.backward()**\n"
   ],
   "id": "2e0f71afe8924dbd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-06T12:05:44.293257Z",
     "start_time": "2025-05-06T12:05:44.285140Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from conda.testing.helpers import TEST_DATA_DIR"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9bee0072aff8f76d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T12:15:13.240342Z",
     "start_time": "2025-05-06T12:15:13.230456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# All elements of target should be between 0 and 1\n",
    "predictions = torch.tensor([0.9, 0.2, 0.8], dtype=torch.float32) # Beispiel: Modellvorhersagen (Wahrscheinlichkeiten)\n",
    "targets = torch.tensor([1.0, 0.0, 0.3], dtype=torch.float32)     # Echte Labels"
   ],
   "id": "4bbeca9022f2e955",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T12:15:14.402391Z",
     "start_time": "2025-05-06T12:15:14.388840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fc = nn.BCELoss()      # Binary Entropy Loss\n",
    "loss = loss_fc(predictions, targets)   # Loss Berechnen\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ],
   "id": "3958af9c61482e2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5074\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ein eigenes Modell (z.‚ÄØB. MyCNN)\n",
    "\n",
    "Datenvorbereitung + DataLoader\n",
    "\n",
    "Loss-Funktion (BCELoss)\n",
    "\n",
    "Optimizer (z.‚ÄØB. Adam)\n",
    "\n",
    "Trainingsschleife"
   ],
   "id": "1318c44b136604bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:44:37.111162Z",
     "start_time": "2025-05-06T13:44:37.098410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- MyCNN\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):                               # Netzwerk definiert und initialisiert\n",
    "        super(MyCNN, self).__init__()                 # Ruft den Konstruktor der Basisklasse nn.Module auf. Das ist n√∂tig, damit PyTorch intern alles korrekt registriert.\n",
    "        # Convolutional Layer-Block\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Erste Convolution-Schicht. Input hat 1 Kanal (Graustufenbild), Output 16 Filter.\n",
    "            nn.ReLU(),                                             # Aktivierungsfunktion ‚Äì f√ºhrt Nichtlinearit√§t ein.\n",
    "            nn.MaxPool2d(2),                                       # Verkleinert die Bildgr√∂√üe um die H√§lfte.\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # Zweite Convolution ‚Äì jetzt 32 Filter.\n",
    "            nn.ReLU(),                                               # Aktivierungsfunktion ‚Äì f√ºhrt Nichtlinearit√§t ein. warum 2te mal\n",
    "            nn.MaxPool2d(2)                                          # Verkleinert die Bildgr√∂√üe um die H√§lfte. warum 2te mal\n",
    "        )\n",
    "        # Fully Connected Layer\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(288, 1),\n",
    "            nn.Sigmoid()  # Wichtig f√ºr BCE!\n",
    "        )\n",
    "    # DAMIT DAS MANN KANN RUFT JEDES MAL BATCH   x output  \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x    "
   ],
   "id": "f8ffaf0b26c0bf1a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Was macht Fully Connected Layer (FC)? Kann es ohne Konvertierung verwendet werden?\n",
    "Seine Funktion: Er fasst alle Merkmale in einem einzigen Vektor zusammen und trifft die endg√ºltige Entscheidung (zum Beispiel: ‚ÄûIst eine Maske vorhanden?‚Äú ‚Üí ja/nein).\n",
    "\n",
    "Mathematisch: Jedes Neuron ist mit allen Neuronen in der vorherigen Schicht verbunden.\n",
    "\n",
    "‚úÖ Ja, es kann ohne Faltung verwendet werden (z. B. MLP ‚Äì Multilayer Perceptron), aber es ist f√ºr Bilder sehr ineffizient, weil:\n",
    "\n",
    "Positionsinformationen gehen verloren.\n",
    "\n",
    "Zu viele Parameter ‚Üí √úberanpassung."
   ],
   "id": "928aeec04f14e78a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:44:39.065149Z",
     "start_time": "2025-05-06T13:44:39.042562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Dummy-Daten: 100 Bilder in Grau (1 Kanal, 28x28)\n",
    "X = torch.randn(100, 1, 28, 28)\n",
    "y = torch.randint(0, 2, (100, 1)).float()  # Bin√§re Labels (0 oder 1)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ],
   "id": "2c7a9e564a7f1b81",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:44:41.155826Z",
     "start_time": "2025-05-06T13:44:40.655067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRAINING\n",
    "model = MyCNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for batch_X, batch_y in loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ],
   "id": "3946c782d1c0bdb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7857\n",
      "Epoch 2, Loss: 0.6931\n",
      "Epoch 3, Loss: 0.6740\n",
      "Epoch 4, Loss: 0.6722\n",
      "Epoch 5, Loss: 0.6950\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† Erwartetes Verlustrisiko (Expected Risk)Das Ziel eines ML-Modells ist, eine Funktion \n",
    "Das Ziel eines ML-Modells ist, eine Funktion \n",
    "ùõø\n",
    "Œ¥ zu finden, die bei gegebenen Eingabedaten \n",
    "ùëñ\n",
    "i eine gute Vorhersage \n",
    "ùëú\n",
    "o macht. Dazu definieren wir:\n",
    "\n",
    "Die Verlustfunktion \\( L(\\hat{o}, (i, o)) \\) misst den Fehler zwischen der Vorhersage \\( \\hat{o} \\) und dem tats√§chlichen Wert \\( o \\), gegeben einer Eingabe \\( i \\). Da die Eingaben aus einer Verteilung stammen, betrachten wir den Erwartungswert des Verlusts √ºber alle m√∂glichen Eingaben:\n",
    "\n",
    "$$\n",
    "R(\\delta) := \\int_{\\Omega} L(\\hat{o}, (i, o)) \\cdot p(\\Omega)\n",
    "$$\n",
    "\n",
    "- \\(R(Œ¥)): Raum der Eingabe-Ausgabe-Paare \\( (i, o) \\)\n",
    "- \\(L(‚ãÖ)): Verlustfunktion, (wie stark weicht die Vorhersage von der Wahrheit ab)\n",
    "- \\(p(Œ©): Wahrscheinlichkeitsdichte f√ºr das Auftreten von \\( (i, o) \\)\n",
    "\n",
    "üëâ Ziel ist es, eine Entscheidungsfunktion \\( \\delta \\) zu finden, die das **Risiko** \\( R(\\delta) \\) minimiert:\n",
    "\n",
    "$$\n",
    "\\delta^* := \\operatorname{argmin}_\\delta R(\\delta)\n",
    "$$\n",
    "\n",
    "Man nennt \\(Œ¥* ) auch die **Bayes'sche Entscheidungsfunktion**, da sie unter der gegebenen Verteilung den geringsten erwarteten Verlust erzielt.\n"
   ],
   "id": "697e504db949d18d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ebff31bf4e6f4980"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Was sind ReLU und MaxPool? Warum MaxPool2d?\n",
    "    üß† ReLU (gleichgerichtete lineare Einheit):\n",
    "            Aktivierungsfunktion: Macht aus negativen Werten 0.\n",
    "\n",
    "            Die Berechnung ist sehr schnell.\n",
    "\n",
    "    Es bringt Nichtlinearit√§t. Andernfalls kann das Modell nicht √ºber das ‚ÄûZeichnen einer Linie‚Äú hinausgehen.\n",
    "\n",
    "    üß± MaxPool2d:\n",
    "            Es komprimiert die Daten, reduziert also die Gr√∂√üe.\n",
    "            \n",
    "            Nimmt 2x2 Regionen und beh√§lt nur den gr√∂√üten Wert.\n",
    "            \n",
    "            Reduziert L√§rm und erm√∂glicht effizientes Lernen.\n",
    "\n",
    "### Warum 2D?\n",
    "\n",
    "Bilder sind 2D (H√∂he √ó Breite), daher verwenden wir MaxPool2d.\n",
    "\n",
    "            MaxPool1d ‚Üí f√ºr Zeitreihen\n",
    "            \n",
    "            MaxPool3d ‚Üí f√ºr 3D-Daten (z. B. Videos)\n",
    "\n",
    "### Warum gibt es nach jeder Faltung ReLU + MaxPool?\n",
    "            Bei der Faltung werden nur Filter angewendet, jedoch keine Nichtlinearit√§t oder Dimensionsreduzierung erreicht.\n",
    "            \n",
    "            ReLU ‚Üí st√§rkt das Lernen (vermeidet Gradientenverlust).\n",
    "            \n",
    "            MaxPool ‚Üí fasst Funktionen zusammen und bietet eine bessere Generalisierung mit weniger Parametern.\n",
    "            \n",
    "            Jedes Mal, wenn dieses Paar angewendet wird, gilt f√ºr das Modell:\n",
    "            \n",
    "            Lernt besser.\n",
    "            \n",
    "            Kann mit weniger Daten verallgemeinern.\n",
    "            \n",
    "            Die Berechnung wird effizienter.\n",
    "            \n",
    "# Lernverfahren"
   ],
   "id": "9d3c724be370957f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1. Perzeptron-Ausgabe (einfaches Neuron mit Aktivierungsfunktion):**\n",
    "$$\n",
    "\\phi(x) = \\rho\\left( \\sum_{j=1}^{n} x_j \\cdot w_{j,i} + b \\right) = x_i\n",
    "$$\n",
    "\n",
    "**2. Gewichtsanpassung im Hopfield-Netzwerk (Hebbsche Lernregel):**\n",
    "$$\n",
    "w(i,j) = \\sum_{k=1}^{n} a_k(i) \\cdot a_k(j)\n",
    "$$\n",
    "\n",
    "**3. Neuron-Ausgabe in Layer \\( l \\) eines Feedforward-Netzes (mit Bias und Aktivierungsfunktion):**\n",
    "$$\n",
    "x_h^{(l)} = \\rho^{(l)}\\left( \\sum_{j=1}^{n^{(l-1)}} x_j^{(l-1)} \\cdot w_{j,h}^{(l)} + b_h^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "**4. Backpropagation ‚Äì Gewichtsanpassung (Gradient Descent Regel):**\n",
    "$$\n",
    "\\Delta w_{j,h}^{(l)} = \\eta \\cdot \\frac{\\partial L}{\\partial w_{j,h}^{(l)}}\n",
    "$$\n",
    "\n",
    "**5. Backpropagation ‚Äì Bias-Anpassung:**\n",
    "$$\n",
    "\\Delta b_h^{(l)} = \\eta \\cdot \\frac{\\partial L}{\\partial b_h^{(l)}}\n",
    "$$\n",
    "\n",
    "**6. Neue Gewichte nach der Aktualisierung (Gewichtsupdate):**\n",
    "$$\n",
    "w_{j,h}^{(l),\\text{neu}} = w_{j,h}^{(l),\\text{alt}} - \\Delta w_{j,h}^{(l)}\n",
    "$$\n",
    "\n",
    "**7. Neue Bias-Werte nach der Aktualisierung:**\n",
    "$$\n",
    "b_h^{(l),\\text{neu}} = b_h^{(l),\\text{alt}} - \\Delta b_h^{(l)}\n",
    "$$\n",
    "\n",
    "**8. Beispiel: √Ñnderung eines Gewichts mit Lernrate \\( \\eta = 0.1 \\):**\n",
    "$$\n",
    "\\Delta w_1 = \\eta \\cdot \\frac{\\partial L}{\\partial w_1} = 0.1 \\cdot (-0.5) = -0.05\n",
    "$$\n",
    "\n",
    "**9. Neues Gewicht nach Update:**\n",
    "$$\n",
    "w_1^{\\text{neu}} = w_1 + \\Delta w_1 = 0.5 - 0.05 = 0.45\n",
    "$$\n",
    "\n",
    "**10. Zweites Beispiel mit positivem Gradienten:**\n",
    "$$\n",
    "\\Delta w_3 = 0.1 \\cdot (+0.05) = +0.005 \\quad \\Rightarrow \\quad w_3^{\\text{neu}} = -0.05 + 0.005 = -0.045\n",
    "$$\n",
    "\n",
    "**11. Allgemeine Gewichtsanpassungsformel (Gradientenverfahren):**\n",
    "$$\n",
    "w_{\\text{neu}} = w_{\\text{alt}} - \\eta \\cdot \\nabla L\n",
    "$$\n",
    "\n"
   ],
   "id": "d774e8287124e64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
